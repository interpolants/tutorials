{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef54535f-8c66-4b92-be97-1ff2d4d00a6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hello! Welcome to a short tutorial on Flow Map Matching\n",
    "\n",
    "Our aims with this tutorial will be to introduce you to numerical realizations of some of the concepts you've been learning about in class. \n",
    "\n",
    "We will build a simple generative model that learns to sample from some 2-dimensional distribution with probability density $\\rho_1$. This generative modeling will come in the form of a *map* $T: \\mathbb R^2 \\rightarrow \\mathbb R^2$ that maps samples from a reference distribution with density $\\rho_0$. This map will be such that for any $x_0 \\sim \\rho_0$, the map gives $T(x_0) = x_1 \\sim \\rho_1$. \n",
    "\n",
    "As you have been discussing in class, this map is given by $T(x) = X_{0,1}(x)$, where $X_{s,t}(x)$ is the *flow map* between times $s$ and $t$, which solves the following ordinary differential equation: \n",
    "\n",
    "$$ \\dot X_{s,t}(x) = b_t(X_{s,t}) \\quad \\text{such that} \\quad  X_{s,s}(x) = x $$\n",
    "\n",
    "where $b_t(x) : [0,1] \\times \\mathbb R^d \\rightarrow \\mathbb R^d$ is a *velocity field* which tells us how the flow map $X_{s,t}(x)$ should instantaneously evolve in magnitude and direction in increments of time $\\Delta t$. \n",
    "\n",
    "<!-- ![alt text](probflow.png \"Flow\") -->\n",
    "\n",
    "\n",
    "\n",
    "Many such flow maps $X_{s,t}(x)$ exist that meet the boundary condition above. We can also ask about the dynamics of the probability density itself endowed by this flow map $X_{s,t}(x)$, rather than just what happens to samples. We say that associatd to $X_t(x)$ is the time dependent density $\\rho(t,x)$ so that the pair solve the transport equation:\n",
    "\n",
    "$$\\partial_t \\rho_t(x) + \\nabla \\cdot \\big [b_t(x) \\rho_t(x) \\big] = 0, \\quad \\text{with} \\quad \\rho_{t=0} = \\rho_0 \\text{ and } \\rho_{t=1} = \\rho_1 $$\n",
    "\n",
    "which says that mass is conserved and the transport starts and ends in the right place. \n",
    "\n",
    "##### For most distributions, we don't know what the flow map $X_{s,t}(x)$ is ahead of time and must try to *estimate* it over a parametric function class. What you'll define below is a way to do this using the method of stochastic interpolants, which gives us a way to sample a time dependent density $\\rho_t(x)$ and learn the $X_{s,t}(x)$ that performs the associated mapping.\n",
    "\n",
    "Let's lay out the steps to do this in code. Here's what we'll need:\n",
    "\n",
    "- a python package that allows us to perform optimization over our parametric functions using autodifferentiation: PyTorch\n",
    "- A way to define and sample under the probability densities $\\rho_0$ and $\\rho_1$\n",
    "- A neural network that we will use as our model of the map $\\hat X_{s,t}(x)$\n",
    "- An implementation of the stochastic interpolant and the associated optimization loop used in conjunction with it to fit $\\hat X_{s,t}(x)$ to the true $\\hat X_{s,t}(x)$.\n",
    "- Some visualizations of the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c719-ddd9-4e2e-881b-5bdb929f2848",
   "metadata": {},
   "source": [
    "### Software pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ae0ef-9c94-456b-a946-081e6beeb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch  # 2.0 or greater\n",
    "    from torch.func import vmap\n",
    "    from torch.func import jacfwd\n",
    "    \n",
    "    print(\"All packages are installed!\")\n",
    "except ImportError as e:\n",
    "    print(f\"An error occurred: {e}. Please make sure all required packages are installed.\")\n",
    "\n",
    "    \n",
    "from typing import Callable\n",
    "import math\n",
    "\n",
    "# from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c2ac6-0ee9-4a0a-978b-a75544c9d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71afcc-3c69-402f-b139-b1e66005100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(var):\n",
    "    return var.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885a272-c226-44df-8d67-e31d37df1002",
   "metadata": {},
   "source": [
    "### Defining and sampling our two distributions\n",
    "\n",
    "To learn a generative model to sample under $\\rho_1$, we first need to define what distribution we want to sample from and what distribution we start from!\n",
    "\n",
    "Let's say that our data is in $\\mathbb R^2$. We will take $\\rho_0$ as a Normal distribution with mean vector $m = (0,0)$ and identity covariance $C = Id$ so that $\\rho_0 \\equiv \\mathsf N(0, Id)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a72cae-fbb9-41ba-9d74-64817a8e79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "class BaseDistribution:\n",
    "    def __init__(self, mean, cov):\n",
    "        self.mean = mean\n",
    "        self.covariance = cov\n",
    "        self.distribution = MultivariateNormal(mean, cov)\n",
    "    \n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Draws $n$ samples from the Gaussian distribution.   \n",
    "        \"\"\"\n",
    "        return self.distribution.sample((n,))\n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Evaluates the log probability of given samples $x$ under the distribution. \n",
    "        \"\"\"\n",
    "        return self.distribution.log_prob(x)\n",
    "\n",
    "mean = torch.tensor([0.0, 0.0])  # \\mu \\in R^2\n",
    "cov = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # \\Sigma \\in R^{2x2}\n",
    "\n",
    "base = BaseDistribution(mean, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac09a5-a918-467a-a49e-2b93796b72b4",
   "metadata": {},
   "source": [
    "#### Let's choose our target distribution to be: a checkerboard in in 2d space or a Gaussian Mixture defined as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597732a-dfa5-4f45-881d-5d94cf793ab4",
   "metadata": {},
   "source": [
    "#### Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e2ef0-4cc3-4e9a-8c10-0e1075d524b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checker\n",
    "ndim = 2\n",
    "def target(bs):\n",
    "    x1 = torch.rand(bs) * 4 - 2\n",
    "    x2_ = torch.rand(bs) - torch.randint(2, (bs,)) * 2\n",
    "    x2 = x2_ + (torch.floor(x1) % 2)\n",
    "    return (torch.cat([x1[:, None], x2[:, None]], 1) * 2)\n",
    "\n",
    "\n",
    "# target = GMM(mus_target, var_target)\n",
    "target_samples  = target(10000)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.hist2d(grab(target_samples[:,0]), grab(target_samples[:,1]), bins = 100, range=[[-9,9],[-9,9]]);\n",
    "plt.title(\"Multimodal Target\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a249252-34d1-4aff-a0f7-1729b51a0a2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mixture of Gaussians, if you want to use it instead as a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374f43e-14d8-4757-bdf3-b1bee396d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.mixture_same_family import MixtureSameFamily\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.independent import Independent\n",
    "from torch.distributions.multivariate_normal import  MultivariateNormal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Prior(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract class for prior distributions of normalizing flows. The interface\n",
    "    is similar to `torch.distributions.distribution.Distribution`, but batching\n",
    "    is treated differently. Parameters passed to constructors are never batched,\n",
    "    but are aware of the target (single) sample shape. The `forward` method then\n",
    "    accepts just the batch size and produces a batch of samples of the known\n",
    "    shape.\n",
    "    \"\"\"\n",
    "    def forward(self, batch_size):\n",
    "        raise NotImplementedError()\n",
    "    def log_prob(self, x):\n",
    "        raise NotImplementedError()\n",
    "    def draw(self, batch_size):\n",
    "        \"\"\"Alias of `forward` to allow explicit calls.\"\"\"\n",
    "        return self.forward(batch_size)\n",
    "    \n",
    "\n",
    "    \n",
    "class GMM(Prior):\n",
    "    def __init__(self, loc=None, var=None, scale = 1.0, ndim = None, nmix= None, device='cpu', requires_grad=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.scale = scale       ### only specify if loc is None\n",
    "        def _compute_mu(ndim):\n",
    "                return self.scale*torch.randn((1,ndim))\n",
    "                        \n",
    "        if loc is None:\n",
    "            self.nmix = nmix\n",
    "            self.ndim = ndim \n",
    "            loc = torch.cat([_compute_mu(ndim) for i in range(1, self.nmix + 1)], dim=0)\n",
    "            var = torch.stack([1.0*torch.ones((ndim,)) for i in range(nmix)])\n",
    "        else:\n",
    "            self.nmix = loc.shape[0]\n",
    "            self.ndim = loc.shape[1] ### locs should have shape [n_mix, ndim]\n",
    "            \n",
    "        self.loc = loc   ### locs should have shape [n_mix, ndim]\n",
    "        self.var = var   ### should have shape [n_mix, ndim]\n",
    "        \n",
    "        if requires_grad:\n",
    "            self.loc.requires_grad_()\n",
    "            self.var.requires_grad_()\n",
    "        \n",
    "        mix = Categorical(torch.ones(self.nmix,))\n",
    "        comp = Independent(Normal(\n",
    "                     self.loc, self.var), 1)\n",
    "        self.dist = MixtureSameFamily(mix, comp)\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        logp = self.dist.log_prob(x)\n",
    "        return logp\n",
    "    \n",
    "    \n",
    "    def forward(self, batch_size):\n",
    "        x = self.dist.sample((batch_size,))\n",
    "        return x\n",
    "    \n",
    "    def rsample(self, batch_size):\n",
    "        x = self.dist.rsample((batch_size,))\n",
    "        return x\n",
    "    \n",
    "nmix = 8\n",
    "ndim = 2\n",
    "def _compute_mu(i):\n",
    "            return 5.0 * torch.Tensor([[\n",
    "                        torch.tensor(i * math.pi / 4).sin(),\n",
    "                        torch.tensor(i * math.pi / 4).cos()]])\n",
    "mus_target = torch.stack([_compute_mu(i) for i in range(nmix)]).squeeze(1)\n",
    "var_target = torch.stack([torch.tensor([0.7, 0.7]) for i in range(nmix)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd36a55-d39d-44d3-a4e7-5cdfc9c5beac",
   "metadata": {},
   "source": [
    "#### Below we can visualize both of the distributions by drawing samples from them and plotting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a2c0b-e20b-4840-a31a-774fdc5e8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5000\n",
    "c = '#62508f' # plot color\n",
    "\n",
    "x0s = base.sample(bs).detach().numpy()\n",
    "x1s = target(bs).detach().numpy()\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "ax0.scatter(x0s[:,0], x0s[:,1], alpha = 0.5, c = c)\n",
    "ax0.set_xlim(-4,4), ax0.set_ylim(-4,4)\n",
    "ax0.set_title(r\"Samples under $\\rho_0$\", fontsize = 16)\n",
    "ax0.set_xticks([-4,0,4]), ax0.set_yticks([-4,0,4])\n",
    "\n",
    "ax1.scatter(x1s[:,0], x1s[:,1], alpha = 0.5, c = c)\n",
    "ax1.set_xlim(-8,8), ax1.set_ylim(-8,8)\n",
    "ax1.set_title(r\"Samples under $\\rho_1$\", fontsize = 16)\n",
    "ax1.set_xticks([-4,0,4]), ax1.set_yticks([-4,0,4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91148d46-e4ee-4170-ab69-404787399b8e",
   "metadata": {},
   "source": [
    "## Building a map between $\\rho_0$ and $\\rho_1$ with the interpolant\n",
    "\n",
    "You have been learning in class that you can use the method of stochastic interpolants to define a time dependent density $\\rho_t(x)$ that connects $\\rho_0$ to $\\rho_1$ and allows us to learn the associated velocity field $b_t(x)$ that would map samples from one to samples from another.\n",
    "\n",
    "Let's introduce the interpolant function here and make some statements about it. The stochastic interpolant $I_t$ is a stochastic process given as:\n",
    "\n",
    "$$ I_t  = \\alpha_t x_0 + \\beta_t x_1 \\quad \\text{where} x_0 \\sim \\rho_0 \\text{ and } x_1 \\sim \\rho_1 $$ \n",
    "\n",
    " - When $x_0, x_1$ are drawn accordingly, we say that the stochastic interpolant $x(t)$ is a sample under the time dependent density i.e. $I_t \\sim \\rho_t(x)$. \n",
    " - The velocity field associated to this $\\rho_t(x)$ is given by the conditional expectation of the time dynamics of the interpolant, namely: \n",
    " \n",
    " $$ b_t(x) = \\mathbb E[ \\dot I_t | I_t = x]$$\n",
    "\n",
    " - The probability flow is the solution to the ODE\n",
    "\n",
    "$$ \\dot X_{s,t}(x) = b_t(X_{s,t}(x)), \\qquad X_{s,s}(x) = x$$\n",
    "where the dot denote deriavtive with respect to $t$.\n",
    "\n",
    " \n",
    "What do we mean when we say $I_t$ samples $\\rho_t(x)$? Well let's take a look by implementing it. We will make a class called `Interpolant` that implements $I_t$ and it's time derivative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c7bee-bb6b-440e-8512-e966d382c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using alpha(t) = (1-t) and beta(t) = t\n",
    "class Interpolant:\n",
    "    def alpha(self, t):\n",
    "        return 1.0 - t\n",
    "    \n",
    "    def dotalpha(self, t):\n",
    "        return -1.0\n",
    "    \n",
    "    def beta(self, t):\n",
    "        return t\n",
    "    \n",
    "    def dotbeta(self, t):\n",
    "        return 1.0\n",
    "    \n",
    "    def _single_xt(self, x0, x1, t):\n",
    "        return self.alpha(t)*x0 + self.beta(t)*x1\n",
    "    \n",
    "    def _single_dtxt(self, x0, x1, t):\n",
    "        return self.dotalpha(t)*x0 + self.dotbeta(t)*x1\n",
    "    \n",
    "    def xt(self, x0, x1, t):\n",
    "        return vmap(self._single_xt, in_dims=(0, 0, 0))(x0,x1,t)\n",
    "    \n",
    "    def dtxt(self, x0, x1, t):\n",
    "        return vmap(self._single_dtxt, in_dims=(0, 0, 0))(x0,x1,t)\n",
    "    \n",
    "    \n",
    "interpolant = Interpolant()\n",
    "\n",
    "bs  = 5000\n",
    "x0s = base.sample(bs)\n",
    "x1s = target(bs)\n",
    "t   = 0.2*torch.ones(bs)\n",
    "\n",
    "xts = interpolant.xt(x0s, x1s, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f45bb-9369-471d-9583-3fc3ae2bd55e",
   "metadata": {},
   "source": [
    "In the above code, we implement the interpolant for a single sample of $x_0, x_1$, and $t$, which have shape `[d], [d], [1]` respectively. Then, we use a tool in pytorch called `vmap` which allows us to generalize the code for arbitrary *batches* of samples of $x_0, x_1$ and $t$. That way, if we have $N$ samples of $x_0, x_1$ and $t$, we can compute the interpolant for all $N$ of them at once using vmap.\n",
    "\n",
    "This means we can feed the function `interpolant.xt` a batch of samples of shape `[N, d]` for $x_0$ and $x_1$ and of shape `[N]` for time.\n",
    "\n",
    "Let's see what happens when we sample $\\rho_t(x)$ at various times along $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5cc80-8473-47fd-84d7-27d9292a79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 5000\n",
    "ncol = 6\n",
    "ts   = torch.linspace(0, 1, ncol)\n",
    "c = '#62508f'\n",
    "\n",
    "fig, axes = plt.subplots(1, ncol, figsize=(ncol*4,4))\n",
    "\n",
    "for i, t in enumerate(ts):\n",
    "    \n",
    "    \n",
    "    tt  = t.repeat(bs)\n",
    "    x0s = base.sample(bs)\n",
    "    x1s = target(bs)\n",
    "    xts = interpolant.xt(x0s, x1s, tt)\n",
    "    \n",
    "    axes[i].scatter(xts[:,0], xts[:,1], alpha = 0.08, c = c) # plot samples x_t \\sim \\rho_t\n",
    "    \n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_title(r'$\\rho_{t = %.1f}$' % t, fontsize = 20, weight='bold')\n",
    "    axes[i].set_xlim(-8,8)\n",
    "    axes[i].set_ylim(-8,8)\n",
    "    \n",
    "    if i !=0:\n",
    "        axes[i].set_yticks([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a1d39-fa3f-4bcc-8755-8ee9156af767",
   "metadata": {},
   "source": [
    "### Neural Network $\\hat X_{s,t}(x)$ to model $X_{s,t}(x)$\n",
    "\n",
    "Now we need to define a neural network which has learnable parameters to model $X_{s,t}(x)$. Recall that this map takes in two time coordinates and a spatial sample which is $\\mathbb R^2$. This means that our neural network approximation $\\hat X_{s,t}(x)$ should be a function that has an input size of $2 + \\text{space dim} = 4$ and should have an output dimension of $\\text{space dim} = 2$. \n",
    "\n",
    "To define a neural network, we'll use PyTorch's `torch.nn.Module` library which allows us to compose parts of the neural network in a way that allows us to take derivatives with respect to the networks weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632f9cd-4b3f-498b-9445-6e502c9477f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import jacrev, grad    \n",
    "    \n",
    "class FlowMap(torch.nn.Module):\n",
    "    # a neural network that takes x in R^d and (s,t) in [0, 1]^2 and outputs a a value in R^d\n",
    "    def __init__(self, d,  hidden_sizes = [256, 256, 256], activation=torch.nn.SiLU):\n",
    "        super(FlowMap, self).__init__()\n",
    "        \n",
    "        \n",
    "        ## feedforward network\n",
    "        layers = []\n",
    "        prev_dim = d + 2  \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(torch.nn.Linear(prev_dim, hidden_size))\n",
    "            layers.append(activation())\n",
    "            prev_dim = hidden_size  \n",
    "\n",
    "        # final layer\n",
    "        layers.append(torch.nn.Linear(prev_dim, d))\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def _single_forward(self, x, s, t):  \n",
    "        s = s.unsqueeze(-1)\n",
    "        t = t.unsqueeze(-1)\n",
    "        return self.net(torch.cat((x, s, t)))\n",
    "    \n",
    "    def forward(self, x, s, t):\n",
    "        return vmap(self._single_forward, in_dims=(0,0,0), out_dims=(0))(x,s,t)\n",
    "    \n",
    "    def _single_Xst(self, x0, s, t):\n",
    "        \"\"\"\n",
    "        A function that implements: X(s,t,x) = (1-(t-s))*x0 + (t-s)*f(s,t,x)\n",
    "        where f(s,t,x) is a neural network.\n",
    "        \n",
    "        This functional form ensures the boundary condition that X(s,s,x) = x0\n",
    "        \"\"\"\n",
    "        return (1-t+s)*x0 + (t-s)*self._single_forward(x0,s,t)\n",
    "    \n",
    "    def Xst(self, x0, s, t):\n",
    "        return vmap(self._single_Xst, in_dims=(0,0,0), out_dims=(0))(x0,s,t)\n",
    "    \n",
    "    def _single_dtXst(self, x0, s, t):\n",
    "        return -x0 + self._single_forward(x0,s,t) + (t-s)*self._single_dt(x0,s,t)   \n",
    "    \n",
    "    def dtXst(self, x0, s, t):\n",
    "        return vmap(self._single_dtXst, in_dims=(0,0,0), out_dims=(0))(x0,s,t)\n",
    "    \n",
    "    def _single_dt(self, x, s, t):\n",
    "        \n",
    "        ## use if using 'cuda' or 'cpu'\n",
    "        # gradt = jacfwd(self._single_forward, 2)(x,s,t)\n",
    "        \n",
    "        ## or approximate by finite difference\n",
    "        ## use if using 'mps'\n",
    "        dt = torch.tensor(0.001)\n",
    "        gradt = .5*(self._single_forward(x,s,t+dt) - self._single_forward(x,s,t-dt))/dt\n",
    "        return gradt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523184e-537d-4589-aa08-e282add07980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MAP:\n",
    "    def __init__(self, T, interpolant, n_step, rev = False):\n",
    "        \n",
    "        self.T           = T\n",
    "        self.interpolant = interpolant\n",
    "        self.n_step      = n_step\n",
    "        self.ts          = torch.linspace(0.0,1.0, n_step + 1).to(device)\n",
    "        if rev == True:\n",
    "            self.ts = torch.flip(self.ts, dims=[0])   \n",
    "    \n",
    "    \n",
    "    def step(self, x, s, t):\n",
    "        return  self.T.Xst(x, s, t) \n",
    "    \n",
    "    def solve(self, x_init):\n",
    "        \n",
    "        bs = x_init.shape[0]\n",
    "        xs = torch.zeros((self.n_step, *x_init.shape)).to(device)\n",
    "        x = x_init\n",
    "        for i,t in enumerate(self.ts[:-1]):\n",
    "            s = t.repeat(len(x)).to(device)\n",
    "            t = self.ts[i+1].repeat(len(x))\n",
    "            # print(s,t)\n",
    "            x = self.step(x,s,t)\n",
    "            xs[i] = x\n",
    "        return xs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a16090-5efe-4d61-9ebd-b1c47ad83ae7",
   "metadata": {},
   "source": [
    "Let's check to make sure our neural network class works by making some fake data and seeing that it outputs something of the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a20d7-a5be-40b7-b6aa-a16ddef4dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "T =  FlowMap(d, hidden_sizes=[512, 512, 512, 512, 512, 512]).to(device)\n",
    "\n",
    "bs = 20000 ## simple test batch size\n",
    "x = torch.rand(bs, d).to(device)\n",
    "s = torch.rand(bs).to(device)\n",
    "t = torch.rand(bs).to(device)\n",
    "\n",
    "print(x.shape)\n",
    "print(s.shape)\n",
    "print(t.shape)\n",
    "\n",
    "t = t.requires_grad_(True)\n",
    "print(t.requires_grad)\n",
    "\n",
    "import time\n",
    "tic = time.perf_counter()\n",
    "out = T.Xst(x,s,t)\n",
    "toc = time.perf_counter()\n",
    "print(\"time with cpu:\", toc-tic, \" sec\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181598a-b2fd-418e-a5ef-2d13e736db65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss function and learning\n",
    "\n",
    "Now that we have a suitable neural network to optimize, we need to specify the learning rule to update the weights of $\\hat X_{s,t}(x)$.\n",
    "\n",
    "Thi map is the unique minimizer of the least squares loss given by:\n",
    "\n",
    "$$ \\mathcal L[\\hat X] = \\int_0^1 \\int_0^1 \\mathbb E_{x_0,x_1}|\\partial_t{\\hat X}_{s,t}(\\hat X_{t,s}(I_t))  - \\dot I_t |^2 ds dt + \\alpha \\int_0^1\\int_0^1  \\mathbb E_{x_0, x_1}|\\hat X_{s,t}(\\hat X_{t,s}(I_t))  - I_t|^2 ds dt$$ where expectation is taken over $x_0, x_1$, and the second term is added to enforce the reversibility (semi-group) constraint $\\hat X_{s,t}(\\hat X_{t,s}(x)) = x$.\n",
    "\n",
    "We may, for performance purposes, only want to learn the map in time intervals `|t-s| < k` for some $k \\in (0,1]$. For example, if $k = 0.25$, then we would be able to use the map in 4 steps, from $0$ to $0.25$, $0.25$ to $0.5$, $0.5$ to $0.75$ and to $1.0$.\n",
    "\n",
    "Let's go ahead and implement this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b27f94-68df-44bf-bb5c-8efc1d6066c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_st(k, size=1000):\n",
    "    # sample s and t uniformly s.t. |s-t| < k\n",
    "    s = torch.rand(size).to(device)\n",
    "    lower_bound = torch.clamp(s - k, min=0)\n",
    "    upper_bound = torch.clamp(s + k, max=1)\n",
    "    t = torch.rand(size).to(device) * (upper_bound - lower_bound) + lower_bound\n",
    "    return s, t\n",
    "\n",
    "ss, ts = get_st(k=0.5, size = 10000)\n",
    "# plt.hist(ss.cpu().numpy(), bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fb133-dcfa-40b1-bcc8-5b03eef1924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _single_loss_fmm(T, interpolant, x0, x1, s, t):\n",
    "    \"\"\"\n",
    "    Interpolant + map matching loss function for a single datapoint of (x0, x1, s, t).\n",
    "    \"\"\"\n",
    "    It          = interpolant._single_xt(x0, x1, t)\n",
    "    dtIt        = interpolant._single_dtxt(x0, x1, t)\n",
    "\n",
    "    Xts = T._single_Xst(It, t, s)\n",
    "    dtXst  = T._single_dtXst(Xts, s, t)\n",
    "    loss1 = ((dtXst - dtIt)**2).sum()\n",
    "\n",
    "    Xst  = T._single_Xst(Xts, s, t)\n",
    "    loss2 = ((Xst - It)**2).sum()\n",
    "    \n",
    "    return loss1+loss2\n",
    "    \n",
    "\n",
    "loss_fn = vmap(_single_loss_fmm, in_dims=(None, None, 0, 0, 0, 0), out_dims=(0), randomness='different')\n",
    "\n",
    "N    = 10\n",
    "x0s  = base.sample(N).to(device)\n",
    "x1s  = target(N).to(device)\n",
    "ts   = torch.rand(N).to(device)\n",
    "ss   = torch.rand(N).to(device)\n",
    "alph = torch.tensor(1.0).to(device)\n",
    "loss_fn(T, interpolant, x0s, x1s, ss, ts).mean()\n",
    "print(ts-ss)\n",
    "print(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094d160-b77a-4784-a885-03a210f43f53",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "Now that we have constructed our loss, let's put it in a loop to interatively update the parameters of $\\hat X_{s,t}(x)$ as we move toward the minimizer of $\\mathcal L[\\hat X]$. \n",
    "\n",
    "To perform this parameter update, we need to introduce a pytorch `optimizer` that performs the gradient update for us. We do that via the following, specifying a learning rate `lr`. We use the `Adam` optimizer, which is a fancier version of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953bc8b5-4224-4921-b9e9-e4843b0b8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4 ## learning rate\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': T.parameters(), 'lr': lr} ])\n",
    "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=2500, gamma=0.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde77806-867f-4969-b47c-1e2a2c77fdc3",
   "metadata": {},
   "source": [
    "Now lets use it in a function called `train_step` which will perform our iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88823d43-3546-48ea-84ac-477c2bdc834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_c(T, interpolant, opt, sched, N, k):\n",
    "    \n",
    "    ## draw N samples form each distribtution and from time uniformly\n",
    "    x0s = base.sample(N).to(device)\n",
    "    x1s = target(N).to(device)\n",
    "    ss, ts = get_st(k=k, size=N)\n",
    "    \n",
    "\n",
    "    # evaluate loss \n",
    "    loss_val = loss_fn(T, interpolant, x0s, x1s, ss, ts).mean()\n",
    "    \n",
    "    # perform backprop\n",
    "    loss_val.backward()\n",
    "    opt.step()\n",
    "    sched.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    res = {\n",
    "            'loss': loss_val.detach().cpu(),\n",
    "        }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d3e02-ac7b-4c62-a8a2-418eda32b326",
   "metadata": {},
   "source": [
    "#### Running the Training\n",
    "\n",
    "We are now ready to train our model. Let's build a loop that runs for `n_opt` steps and store the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717645c3-bf14-4f21-9317-1586812946c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3c343-2c52-444c-a4c3-2f117feab37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_opt = 50000\n",
    "bs    = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b51a70-3d13-4a46-b549-ec0a7a524d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(n_opt))\n",
    "for i in pbar:\n",
    "\n",
    "    res = train_step_c(T, interpolant, opt, sched, bs, k = 0.25)\n",
    "    loss = res['loss'].detach().numpy().mean()\n",
    "    \n",
    "    losses_c.append(loss)\n",
    "    pbar.set_description(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a1f24-ceeb-483b-b6e0-7bac12952e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_c)\n",
    "# plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e9eb1-aea5-46a3-9c07-fc197cc64755",
   "metadata": {},
   "source": [
    "### Iterating the map\n",
    "\n",
    "Now that we've learned $X_{s,t}(x)$, we can iterate upon it to generate data, using a fixed number of iterations. We can also iterate backward to check that the map is reversible and satisfies $\\hat X_{1,0}(\\hat X_{0,1}(x)) = x$. \n",
    "\n",
    "We will do this with the following class called `ODE` which implements the iterative step and performs the rollout over the interval $t \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36f39d-d205-47dc-adc2-0cc57845d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAP:\n",
    "    def __init__(self, T, interpolant, n_step, rev = False):\n",
    "        \n",
    "        self.T           = T\n",
    "        self.interpolant = interpolant\n",
    "        self.n_step      = n_step\n",
    "        self.ts          = torch.linspace(0.0,1.0, n_step + 1).to(device)\n",
    "        if rev == True:\n",
    "            self.ts = torch.flip(self.ts, dims=[0])   \n",
    "    \n",
    "    \n",
    "    def step(self, x, s, t):\n",
    "        return  self.T.Xst(x, s, t) \n",
    "    \n",
    "    def solve(self, x_init):\n",
    "        \n",
    "        bs = x_init.shape[0]\n",
    "        xs = torch.zeros((self.n_step, *x_init.shape)).to(device)\n",
    "        x = x_init\n",
    "        for i,t in enumerate(self.ts[:-1]):\n",
    "            s = t.repeat(len(x)).to(device)\n",
    "            t = self.ts[i+1].repeat(len(x))\n",
    "            # print(s,t)\n",
    "            x = self.step(x,s,t)\n",
    "            xs[i] = x\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a706d6a-5420-4701-b153-00114b2b35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = base.sample(20000).to(device)\n",
    "T = T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd1e07-b524-4b5c-83a0-b2560c35630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nstep1 = 4\n",
    "map1  = MAP(T, interpolant, n_step = nstep1)\n",
    "xfs    = map1.solve(x_init)\n",
    "x1s = xfs[-1].clone().detach().cpu().numpy()\n",
    "\n",
    "map1  = MAP(T, interpolant, n_step = nstep1, rev = True)\n",
    "xfs    = map1.solve(xfs[-1])\n",
    "x0s = xfs[-1].detach().cpu().numpy()\n",
    "\n",
    "s0 = 0.0*torch.zeros(20000).to(device)\n",
    "t1 = 1.0*torch.ones(20000).to(device)\n",
    "\n",
    "tt  = 1.0*torch.ones(20000)\n",
    "x0se = base.sample(20000)\n",
    "x1se = target(20000)\n",
    "xtse = interpolant.xt(x0se, x1se, tt)\n",
    "\n",
    "\n",
    "c = '#62508f' # plot color\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "axes[0].scatter(x1s[:,0], x1s[:,1], alpha = 0.03, c = c)\n",
    "axes[0].set_title(r\"Samples from $\\hat X_{0,1}(x_0)$\", fontsize = 18)\n",
    "axes[0].set_xticks([-4,0,4]), axes[1].set_yticks([]);\n",
    "\n",
    "axes[1].scatter(xtse[:,0], xtse[:,1], alpha = 0.03, c = c)\n",
    "axes[1].set_title(r\"Samples from $x_1$\", fontsize = 18)\n",
    "axes[1].set_xticks([-4,0,4]), axes[1].set_yticks([]);\n",
    "\n",
    "axes[2].scatter(x_init[:20,0].cpu(), x_init[:20,1].cpu(), alpha = 0.5)\n",
    "axes[2].scatter(x0s[:20,0], x0s[:20,1], alpha = 0.5)\n",
    "axes[2].set_title(r\"Samples from $\\hat X_{1,0}(\\hat X_{0,1}(x_0)) = x_0$\", fontsize = 18)\n",
    "axes[2].set_xticks([-4,0,4]), axes[1].set_yticks([]);\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cdc1ae-c815-4b14-847b-dd68207619b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
